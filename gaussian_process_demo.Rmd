---
title: "Gaussian Process Demo"
author: "David Arthur, Krishnan Raman"
date: "November 2, 2020"
output: pdf_document
---

Let $Y(t) = f(t) + \epsilon(t)$ where $\epsilon(t) \sim N(0, \sigma^2)$

We put a prior over the entire function $f(t)$. This is a Gaussian Process prior.

$$f(t) \sim \mathcal{N}(0, \tau^2 K(t, t'))$$

where $K(t, t')$ is the squared exponential covariance function $\text{exp}\Big(-\frac{|t-t'|^2}{2l^2}\Big)$
Let $\boldsymbol{t}$ be the vector of time points where we have observed the process and let $\boldsymbol{t'}$ be the vector of time points where we haven't observed the process and for which we wish to make predictions.

Since $Y(\boldsymbol{t}) | f(\boldsymbol{t})$, $f(\boldsymbol{t})$ is normal and $f(\boldsymbol{t'})$ is normal, it can be shown that:

$$
\begin{bmatrix}
  Y(\boldsymbol{t}) \\
  f(\boldsymbol{t})
\end{bmatrix}
\sim
\mathcal{N}
\Bigg(\begin{bmatrix}
  \boldsymbol{0} \\
  \boldsymbol{0}
\end{bmatrix},
\begin{bmatrix}
  \sigma^2 I_n + \Sigma_{\boldsymbol{t}} & \Sigma_{\boldsymbol{t}} \\
  \Sigma_{\boldsymbol{t}} & \Sigma_{\boldsymbol{t}}
\end{bmatrix}\Bigg)
$$

where $\Sigma_f = \tau^2 K(t, t')$ calculated at each pair of time points in $\boldsymbol{t}$. Using properties of Gaussians, one can show that the posterior of $f(\boldsymbol{t})$ is

$$f(\boldsymbol{t}) | Y(\boldsymbol{t}) \sim \mathcal{N}(\Sigma_{\boldsymbol{t}}(\sigma^2 I_n + \Sigma_{\boldsymbol{t}})^{-1}Y(\boldsymbol{t}), \Sigma_{\boldsymbol{t}} - \Sigma_{\boldsymbol{t}}(\sigma^2 I_n + \Sigma_{\boldsymbol{t}})^{-1}\Sigma_{\boldsymbol{t}})$$

Similarly the posterior predictive distribution for $f(\boldsymbol{t'})$ is

$$f(\boldsymbol{t'}) | Y(\boldsymbol{t}) \sim \mathcal{N}(\Sigma_{\boldsymbol{t'},\boldsymbol{t}}(\sigma^2 I_n + \Sigma_{\boldsymbol{t}})^{-1}Y(\boldsymbol{t}), \Sigma_{\boldsymbol{t}} - \Sigma_{\boldsymbol{t'},\boldsymbol{t}}(\sigma^2 I_n + \Sigma_{\boldsymbol{t}})^{-1}\Sigma_{\boldsymbol{t},\boldsymbol{t'}})$$

Using the following priors:

$\sigma^2 \sim \text{IG}(a_{\sigma^2}, b_{\sigma^2})$

$\tau^2 \sim \text{IG}(a_{\tau^2}, b_{\tau^2)}$

we got the following conditional distributions for $\sigma^2$ and $\tau^2$:

$\sigma^2 | \cdot \sim \text{IG}(a_{\sigma^2} + n/2, b_{\sigma^2} + 0.5 (Y(\boldsymbol{t})-f(\boldsymbol{t}))'(Y(\boldsymbol{t})-f(\boldsymbol{t}))$

$\tau^2 | \cdot \sim \text{IG}(a_{\tau^2} + n/2, b_{\tau^2} + 0.5 (f(\boldsymbol{t}))'{K_{\boldsymbol{t}}}^{-1}(f(\boldsymbol{t}))$

With the prior for $l^2$ of $l^2 \sim \text{G}(a_{l^2}, b_{l^2})$, then if we propose a new $l^2$ from a normal distribution centered at the old $l^2$, we accept this proposal with probability

$\text{min}\Bigg\{1, \frac{p(f(\boldsymbol{t})|(l^2)^*)p((l^2)^*)}{p(f(\boldsymbol{t})|l^2)p(l^2)}\Bigg\}$

The code for this Gibbs-Sampling scheme with the Metropolis step for $l^2$ is below.

# Temporal Gaussian Process

```{r}
library(mvtnorm)
library(GauPro)

# Function to compute the squared exponential covariance function for given time points

kern <- function(tt, l2){
  D <- as.matrix(dist(tt, diag = TRUE, upper = TRUE))^2
  exp(-1/(2*l2)*D)
}

# Create time series vector of length 200

n <- 200
tt <- seq(-2*pi, 2*pi, length = n)

# Identity matrix for variance of errors

In <- diag(1, n)

# Identity matrix used for numerical stability when inverting large matrix

In_pert <- diag(0.0001, n)

# True variance and range parameters

sig2 <- 0.5
tau2 <- 1
l2 <- 1.47

# Our observed data will be a subset of 100 observations from the true process

n_sub <- 100
In_sub <- diag(1, n_sub)

ind <- sort(sample(1:n, n_sub))
tt_sub <- tt[ind]
tt_pred <- tt[-ind]

# Observed data is a periodic wave function plus normal noise

Y <- sin(tt_sub) + rnorm(n_sub, 0, sqrt(sig2))

plot(tt_sub, Y, main = "Observed Data", xlab = "Time", ylab = "Value")

# Number of iterations for MCMC sampling

niter <- 1000

# Save draws from posterior here

# f_save is posterior of function at points where we have observed the function
# fpred_save is predictive posterior for points of function where we haven't observed observations

f_save <- matrix(0, niter, n_sub)
fpred_save <- matrix(0, niter, n-n_sub)
sig2_save <- rep(0, niter)
tau2_save <- rep(0, niter)

# Initial values

sig2_save[1] <- sig2
tau2_save[1] <- tau2

# Values for hyperparameters

sig2_a <- 2
sig2_b <- 2
tau2_a <- 2
tau2_b <- 2

# MCMC Sampler

for (i in 2:niter){
  K <- tau2_save[i-1]*kern(tt, l2) + diag(10e-06, n)
  Kf <- K[ind, ind]
  Kfpred <- K[-ind, ind]
  Kpred <- K[-ind, -ind]

  YSig_inv <- solve(sig2_save[i-1]*In_sub + Kf, In_sub)

  Kf_Sigma <- Kf - Kf%*%YSig_inv%*%t(Kf)
  Kpred_Sigma <- Kpred - Kfpred%*%YSig_inv%*%t(Kfpred)

  Kf_Mu <- Kf%*%YSig_inv%*%Y
  Kpred_Mu <- Kfpred%*%YSig_inv%*%Y

  f_save[i,] <- rmvnorm(1, Kf_Mu, Kf_Sigma)
  fpred_save[i,] <- rmvnorm(1, Kpred_Mu, Kpred_Sigma)

  sig2_save[i] <- 1/rgamma(1, n_sub/2 + sig2_a, 
                           sig2_b + 0.5*t(Y-f_save[i,])%*%(Y-f_save[i,]))
  tau2_save[i] <- 1/rgamma(1, n_sub/2 + tau2_a, 
                           tau2_b + 0.5*t(f_save[i,])%*%solve(K[ind,ind], f_save[i,]))
}

burn <- 100

f_pred <- colMeans(f_save[burn:niter,])
fstar_pred <- colMeans(fpred_save[burn:niter,])

fcomb_pred <- rep(0, n)
fcomb_pred[ind] <- f_pred
fcomb_pred[-ind] <- fstar_pred

# Get fit from an R package just to compare our fit

gaupro_fit <- GauPro(tt_sub, Y)
gaupro_preds <- gaupro_fit$predict(tt)

plot(tt_sub, Y, main = "Fitted Function from Gaussian Process",
     xlab = "Time", ylab = "Value")
lines(tt, fcomb_pred, col = 'red')
lines(tt, sin(tt), col = 'blue')
lines(tt, gaupro_preds, col = "green")

legend("topright", c("True", "Our Fit", "GauPro Fit"),
       col = c("blue", "red", "green"), lwd = c(2,2,2))
```

# Spatio-Temporal Gaussian Process

```{r}
set.seed(2)

space.kern <- function(D, l2){
  exp(-1/(2*l2)*D)
}

space.D <- function(lat_long){
  D <- as.matrix(dist(lat_long, diag = TRUE, upper = TRUE))^2
}

time.kern <- function(D, l2){
  exp(-1/(2*l2)*D)
}

time.D <- function(tt){
  D <- as.matrix(dist(tt, diag = TRUE, upper = TRUE))^2
}

nloc <- 3
ntime <- 200
ntime_sub <- 100
N <- ntime*nloc
Nsub <- ntime_sub*nloc

lat_long <- cbind(runif(nloc, 35, 45),
                  runif(nloc, 65, 75))

tt <- seq(0, 24-24/ntime, length = ntime)
tt <- tt-mean(tt)
ind <- seq(1, length(tt), by = 2)
ind_all <- c(ind, ind+ntime, ind+2*ntime)
tt_sub <- tt[ind]

l2_time <- 0.5
l2_space <- 10

sig2_time <- 4
sig2_space <- 2
sig2 <- 0.5

D_time <- time.D(tt)
D_space <- space.D(lat_long)

K_time_true <- sig2_time*time.kern(D_time, l2_time) + diag(10e-6, ntime)
K_space_true <- sig2_space*space.kern(D_space, l2_space) + diag(10e-6, nloc)

f_time <- t(rmvnorm(1, rep(0, ntime), K_time_true))
ts.plot(f_time)

K <- kronecker(K_space_true, K_time_true)

f <- t(rmvnorm(1, rep(0, N), K))
ts.plot(f)

fmat <- matrix(f, nrow = ntime, ncol = nloc, byrow = FALSE)
ts.plot(fmat, col = 2:(nloc+1))

Y <- f[ind_all] + rnorm(Nsub, 0, sqrt(sig2))

Ymat <- matrix(Y, nrow = ntime_sub, ncol = nloc, byrow = FALSE)
ts.plot(Ymat, col = 2:(nloc+1))

niter <- 1000

fsave <- matrix(0, niter, Nsub)
sig2_save <- rep(0, niter)
sig2_time_save <- rep(0, niter)
sig2_space_save <- rep(0, niter)

sig2_save[1] <- 2
sig2_time_save[1] <- 2
sig2_space_save[1] <- 2

sig2_a <- 2
sig2_b <- 2
sig2_time_a <- 2
sig2_time_b <- 2
sig2_space_a <- 2
sig2_space_b <- 2

D_space <- space.D(lat_long)
D_time <- time.D(tt_sub)
K_space <- space.kern(D_space, l2_space)
K_time <- time.kern(D_time, l2_time) + diag(10e-02, ntime_sub)
K_space_inv <- solve(K_space)
K_time_inv <- solve(K_time)

for (i in 2:niter){
  K_space_time <- kronecker(sig2_space_save[i-1]*K_space,
                       sig2_time_save[i-1]*K_time)
  sig2I <- diag(sig2_save[i-1], Nsub)
  f_Sigma <- K_space_time + sig2I
  
  f_post_Mu <- K_space_time%*%solve(f_Sigma, Y)
  f_post_Sigma <- K_space_time - K_space_time%*%solve(f_Sigma, K_space_time)
  
  fsave[i,] <- rmvnorm(1, f_post_Mu, f_post_Sigma)
  
  sig2_save[i] <- 1/rgamma(1, sig2_a + Nsub/2, 
                           sig2_b + 0.5*t(Y-fsave[i,])%*%(Y-fsave[i,]))
  
  sig2_space_Sigma <- kronecker(K_space_inv, 1/sig2_time_save[i-1]*K_time_inv)
  
  sig2_space_save[i] <- 1/rgamma(1, sig2_space_a + Nsub/2, 
                              sig2_space_b + 0.5*t(fsave[i,])%*%sig2_space_Sigma%*%fsave[i,])
  
  sig2_time_Sigma <- kronecker(1/sig2_space_save[i]*K_space_inv, K_time_inv)
  
  sig2_time_save[i] <- 1/rgamma(1, sig2_time_a + Nsub/2, 
                                sig2_time_b + 0.5*t(fsave[i,])%*%sig2_time_Sigma%*%fsave[i,])
}

f_pred <- matrix(0, niter, nloc*(ntime-ntime_sub))
In_sub <- diag(ntime_sub*nloc)

for (i in 1:niter){
  K <- kronecker(sig2_space_save[i]*K_space_true, sig2_time_save[i]*K_time_true)
  Kf <- K[ind_all, ind_all]
  Kfpred <- K[-ind_all, ind_all]
  Kpred <- K[-ind_all, -ind_all]

  YSig_inv <- solve(sig2_save[i]*In_sub + Kf, In_sub)
  Kpred_Sigma <- Kpred - Kfpred%*%YSig_inv%*%t(Kfpred)
  Kpred_Mu <- Kfpred%*%YSig_inv%*%Y
  
  f_pred[i,] <- rmvnorm(1, Kpred_Mu, Kpred_Sigma)
}

f_pred_all <- rep(0, N)
f_pred_all[ind_all] <- colMeans(fsave)
f_pred_all[-ind_all] <- colMeans(f_pred)

plot(tt, f_pred_all[1:200], type = 'l')
lines(tt, f[1:200], col = 'red')

plot(tt, f_pred_all[201:400], type = 'l')
lines(tt, f[201:400], col = 'red')

plot(tt, f_pred_all[401:600], type = 'l')
lines(tt, f[401:600], col = 'red')
```

